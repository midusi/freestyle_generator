{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Syllables.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKvzW_8xduPp"
      },
      "source": [
        "Separador de silabas extraido de https://github.com/mabodo/sibilizador/blob/master/Silabizator.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU3q-Hvfdtfh"
      },
      "source": [
        "class char():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "class char_line():\n",
        "    def __init__(self, word):\n",
        "        self.word = word\n",
        "        self.char_line = [(char, self.char_type(char)) for char in word]\n",
        "        self.type_line = ''.join(chartype for char, chartype in self.char_line)\n",
        "        \n",
        "    def char_type(self, char):\n",
        "        if char in set(['a', 'á', 'e', 'é','o', 'ó', 'í', 'ú']):\n",
        "            return 'V' #strong vowel\n",
        "        if char in set(['i', 'u']):\n",
        "            return 'v' #week vowel\n",
        "        if char=='x':\n",
        "            return 'x'\n",
        "        if char=='s':\n",
        "            return 's'\n",
        "        else:\n",
        "            return 'c'\n",
        "            \n",
        "    def find(self, finder):\n",
        "        return self.type_line.find(finder)\n",
        "        \n",
        "    def split(self, pos, where):\n",
        "        return char_line(self.word[0:pos+where]), char_line(self.word[pos+where:])\n",
        "    \n",
        "    def split_by(self, finder, where):\n",
        "        split_point = self.find(finder)\n",
        "        if split_point!=-1:\n",
        "            chl1, chl2 = self.split(split_point, where)\n",
        "            return chl1, chl2\n",
        "        return self, False\n",
        "     \n",
        "    def __str__(self):\n",
        "        return '<'+self.word+':'+self.type_line+'>'\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return '<'+repr(self.word)+':'+self.type_line+'>'\n",
        "\n",
        "class silabizer():\n",
        "    def __init__(self):\n",
        "        self.grammar = []\n",
        "        \n",
        "    def split(self, chars):\n",
        "        rules  = [('VV',1), ('cccc',2), ('xcc',1), ('ccx',2), ('csc',2), ('xc',1), ('cc',1), ('vcc',2), ('Vcc',2), ('sc',1), ('cs',1),('Vc',1), ('vc',1), ('Vs',1), ('vs',1), ('vxv',1), ('VxV',1), ('vxV',1), ('Vxv',1)]\n",
        "        for split_rule, where in rules:\n",
        "            first, second = chars.split_by(split_rule,where)\n",
        "            if second:\n",
        "                if first.type_line in set(['c','s','x','cs']) or second.type_line in set(['c','s','x','cs']):\n",
        "                    #print 'skip1', first.word, second.word, split_rule, chars.type_line\n",
        "                    continue\n",
        "                if first.type_line[-1]=='c' and second.word[0] in set(['l','r']):\n",
        "                    continue\n",
        "                if first.word[-1]=='l' and second.word[-1]=='l':\n",
        "                    continue\n",
        "                if first.word[-1]=='r' and second.word[-1]=='r':\n",
        "                    continue\n",
        "                if first.word[-1]=='c' and second.word[-1]=='h':\n",
        "                    continue\n",
        "                return self.split(first)+self.split(second)\n",
        "        return [chars]\n",
        "        \n",
        "    def __call__(self, word):\n",
        "        return list(map(lambda x: x.word, self.split(char_line(word))))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzNvOk7Momin"
      },
      "source": [
        "Creacion de set de entrenamiento (sentences, next_words)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETykDVQOoj44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88f02ba2-f507-487d-c767-d60bfffdd4c5"
      },
      "source": [
        "\n",
        "# Parameters: change to experiment different configurations\n",
        "SEQUENCE_LEN = 10\n",
        "MIN_WORD_FREQUENCY = 3\n",
        "STEP = 1\n",
        "\n",
        "import os,io\n",
        "import codecs\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "\n",
        "\n",
        "\n",
        "def print_vocabulary(words_file_path, words_set):\n",
        "    words_file = codecs.open(words_file_path, 'w', encoding='utf8')\n",
        "    for w in words_set:\n",
        "        if w != \"\\n\":\n",
        "            words_file.write(w+\"\\n\")\n",
        "        else:\n",
        "            words_file.write(w)\n",
        "    words_file.close()\n",
        "\n",
        "def shuffle_and_split_training_set(sentences_original, next_original, percentage_test=2):\n",
        "    # shuffle at unison\n",
        "    print('Shuffling sentences')\n",
        "\n",
        "    tmp_sentences = []\n",
        "    tmp_next_word = []\n",
        "    for i in np.random.permutation(len(sentences_original)):\n",
        "        tmp_sentences.append(sentences_original[i])\n",
        "        tmp_next_word.append(next_original[i])\n",
        "\n",
        "    cut_index = int(len(sentences_original) * (1.-(percentage_test/100.)))\n",
        "    x_train, x_test = tmp_sentences[:cut_index], tmp_sentences[cut_index:]\n",
        "    y_train, y_test = tmp_next_word[:cut_index], tmp_next_word[cut_index:]\n",
        "\n",
        "    print(\"Size of training set = %d\" % len(x_train))\n",
        "    print(\"Size of test set = %d\" % len(y_test))\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "\n",
        "\n",
        "corpus = \"corpus.txt\"\n",
        "examples = \"examples.txt\"\n",
        "vocabulary = \"vocabulary.txt\"\n",
        "\n",
        "if not os.path.isdir('./checkpoints/'):\n",
        "    os.makedirs('./checkpoints/')\n",
        "\n",
        "symbols = ['\\n','?','¿',',','.','\"',':',\"'\",'(',')']\n",
        "with io.open(corpus, encoding='utf-8') as f:\n",
        "    text = f.read().lower().replace('\\xa0', ' ').replace('-','')\n",
        "    for s in symbols:\n",
        "        text = text.replace(s,' '+s+' ')\n",
        "\n",
        "text = unicodedata.normalize('NFC',text)\n",
        "print('Corpus length in characters:', len(text))\n",
        "\n",
        "\n",
        "# Separar por palabras\n",
        "text_in_words = [w for w in text.split(' ') if (w.strip() != '' or w == '\\n') and w != \"(...)\"]\n",
        "\n",
        "#Separar por silabas\n",
        "s = silabizer()\n",
        "text_in_words = list(map(s,text_in_words))\n",
        "\n",
        "flat_list = []\n",
        "for i, word in enumerate(text_in_words):\n",
        "    for sylab in word:\n",
        "        flat_list.append(sylab)\n",
        "    if (word != [\"\\n\"] and (i+1 < len(text_in_words) and text_in_words[i+1] != [\"\\n\"])):\n",
        "        flat_list.append('\\ ')\n",
        "\n",
        "text_in_words = flat_list\n",
        "\n",
        "print(text_in_words)\n",
        "print('Corpus length in words:', len(text_in_words))\n",
        "\n",
        "# Calculate word frequency\n",
        "word_freq = {}\n",
        "for word in text_in_words:\n",
        "    word_freq[word] = word_freq.get(word, 0) + 1\n",
        "\n",
        "ignored_words = set()\n",
        "for k, v in word_freq.items():\n",
        "    if word_freq[k] < MIN_WORD_FREQUENCY:\n",
        "        ignored_words.add(k)\n",
        "\n",
        "words = set(text_in_words)\n",
        "print('Unique words before ignoring:', len(words))\n",
        "print('Ignoring words with frequency <', MIN_WORD_FREQUENCY)\n",
        "words = sorted(set(words) - ignored_words)\n",
        "print(words)\n",
        "print('Unique words after ignoring:', len(words))\n",
        "print_vocabulary(vocabulary, words)\n",
        "\n",
        "word_indices = dict((c, i) for i, c in enumerate(words))\n",
        "indices_word = dict((i, c) for i, c in enumerate(words))\n",
        "\n",
        "# cut the text in semi-redundant sequences of SEQUENCE_LEN words\n",
        "sentences = []\n",
        "next_words = []\n",
        "ignored = 0\n",
        "for i in range(0, len(text_in_words) - SEQUENCE_LEN, STEP):\n",
        "    # Only add the sequences where no word is in ignored_words\n",
        "    if len(set(text_in_words[i: i+SEQUENCE_LEN+1]).intersection(ignored_words)) == 0:\n",
        "        sentences.append(text_in_words[i: i + SEQUENCE_LEN])\n",
        "        next_words.append(text_in_words[i + SEQUENCE_LEN])\n",
        "    else:\n",
        "        ignored = ignored + 1\n",
        "print('Ignored sequences:', ignored)\n",
        "print('Remaining sequences:', len(sentences))\n",
        "\n",
        "# x, y, x_test, y_test\n",
        "(sentences, next_words), (sentences_test, next_words_test) = shuffle_and_split_training_set(\n",
        "    sentences, next_words\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus length in characters: 2581500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Unique words before ignoring: 4287\n",
            "Ignoring words with frequency < 3\n",
            "['\\n', '!', '!!', '!!!', '!;', '\"', '#mo', '&am', \"'\", '(', ')', ',', '.', '/7', '0', '00', '02', '06', '07', '08', '0k', '1', '10', '100', '11', '12', '120', '13', '14', '15', '16', '17', '18', '180', '19', '2', '20', '200', '21', '22', '23', '24', '25', '26', '27', '2pac', '3', '30', '300', '38', '4', '40', '44', '47', '4am', '5', '50', '542', '6', '60', '69', '6am', '7', '77', '8', '80', '9', '90', ':', ';', '?', '\\\\ ', '_', '__', '___', 'a', 'a!', 'a;', 'ab', 'abs', 'ac', 'ad', 'af', 'ag', 'ah', 'ai', 'air', 'ak', 'al', 'als', 'am', 'an', 'ans', 'ap', 'ar', 'ars', 'as', 'at', 'ats', 'au', 'auau', 'auh', 'aun', 'aus', 'auu', 'au…', 'ay', 'az', 'a—', 'b', 'b;', 'ba', 'ba!', 'bab', 'bac', 'bad', 'bai', 'bain', 'bal', 'bam', 'ban', 'bap', 'bar', 'bars', 'bas', 'bat', 'bau', 'baul', 'bay', 'bb', 'bby', 'bc', 'bdm', 'be', 'bec', 'bel', 'bem', 'ben', 'ber', 'bers', 'bes', 'bet', 'bha', 'bi', 'bia', 'bial', 'bian', 'biar', 'bias', 'bic', 'bie', 'bien', 'bier', 'bies', 'big', 'bin', 'bio', 'bios', 'bir', 'bis', 'bit', 'biz', 'bié', 'bién', 'bió', 'bla', 'blai', 'blam', 'blan', 'blar', 'blas', 'ble', 'blem', 'blen', 'bles', 'bli', 'blia', 'blic', 'blim', 'blin', 'blis', 'blo', 'bloc', 'blon', 'blos', 'blow', 'blu', 'blue', 'blues', 'blun', 'blán', 'blás', 'blé', 'blés', 'bló', 'bm', 'bo', 'bob', 'boc', 'boi', 'bok', 'bol', 'bom', 'bon', 'bor', 'bos', 'bot', 'bou', 'boun', 'bout', 'bow', 'box', 'boy', 'boys', 'bra', 'brain', 'bran', 'brar', 'bras', 'bre', 'brei', 'bren', 'bres', 'bri', 'bria', 'brie', 'brien', 'bril', 'brin', 'brio', 'brir', 'bris', 'brit', 'brió', 'bro', 'broc', 'bron', 'bros', 'brot', 'brr', 'brrr', 'brrum', 'bru', 'brus', 'brá', 'brán', 'brás', 'bré', 'brí', 'bró', 'brón', 'bu', 'bub', 'buc', 'bud', 'bue', 'buen', 'buf', 'bui', 'bul', 'bum', 'bun', 'bur', 'bus', 'but', 'buu', 'buy', 'by', 'bye', 'bys', 'bá', 'bár', 'bé', 'béis', 'bés', 'bí', 'bís', 'bó', 'bón', 'bór', 'bú', 'bús', 'c', 'ca', 'ca!', 'cac', 'cai', 'cal', 'cam', 'can', 'cap', 'car', 'cas', 'cau', 'cay', 'caz', 'cc', 'cd', 'ce', 'ce;', 'ced', 'cei', 'cek', 'cel', 'cen', 'cep', 'cer', 'ces', 'cet', 'ch', 'cha', 'chai', 'chain', 'chains', 'cham', 'chan', 'char', 'chas', 'chat', 'chau', 'che', 'chec', 'chek', 'chen', 'ches', 'chet', 'chez', 'chi', 'chic', 'chie', 'chin', 'chip', 'chips', 'chis', 'cho', 'choc', 'chom', 'chos', 'chris', 'chu', 'chuc', 'chui', 'ché', 'chó', 'chón', 'ci', 'ci!', 'cia', 'cial', 'cian', 'ciar', 'cias', 'cid', 'cie', 'ciem', 'cien', 'cier', 'cil', 'cin', 'cio', 'cioc', 'cion', 'cios', 'cir', 'cis', 'cit', 'ciu', 'cié', 'cién', 'ció', 'ción', 'ci—', 'ck', 'cks', 'cl', 'cla', 'clan', 'clap', 'clar', 'clas', 'clau', 'claus', 'cle', 'cli', 'clic', 'clien', 'clip', 'clo', 'clon', 'cloud', 'clu', 'club', 'clá', 'clí', 'cm', 'co', 'co!', 'coc', 'coi', 'col', 'com', 'con', 'cons', 'cop', 'cops', 'cor', 'cos', 'cot', 'coul', 'coun', 'cow', 'coz', 'cr', 'cr1', 'cra', 'crac', 'craz', 'cre', 'crei', 'cres', 'crew', 'crez', 'cri', 'cria', 'crip', 'cris', 'crió', 'cro', 'crop', 'cros', 'cru', 'crue', 'cruel', 'cruz', 'crá', 'cré', 'crí', 'cró', 'ct', 'cts', 'cu', 'cua', 'cual', 'cuan', 'cuar', 'cuas', 'cuc', 'cue', 'cuel', 'cuen', 'cuer', 'cues', 'cui', 'cul', 'cum', 'cun', 'cuo', 'cup', 'cur', 'cuz', 'cuá', 'cuál', 'cuán', 'cuén', 'cuér', 'cuí', 'cy', 'cyber', 'cá', 'cál', 'cám', 'cán', 'cár', 'cás', 'cé', 'cén', 'cér', 'cés', 'cí', 'cír', 'cís', 'có', 'cón', 'cóp', 'cór', 'cú', 'cúc', 'cúl', 'cús', 'd', 'da', 'da!', 'dac', 'dad', 'dai', 'dak', 'dal', 'dan', 'dap', 'dar', 'das', 'dau', 'day', 'days', 'dc', 'dco', 'dd', 'de', 'de!', 'ded', 'del', 'dem', 'den', 'dep', 'der', 'ders', 'des', 'deu', 'dez', 'dfie', 'dha', 'di', 'dia', 'dial', 'dian', 'diar', 'dias', 'diaz', 'dic', 'die', 'dien', 'dies', 'diez', 'dig', 'din', 'dio', 'dior', 'dios', 'dir', 'dis', 'diz', 'dién', 'dió', 'diós', 'dj', 'dle', 'dm', 'dn', 'do', 'do!', 'doc', 'dog', 'doi', 'doin', 'dol', 'dom', 'don', 'dor', 'dos', 'dou', 'dow', 'doy', 'do\\u2005', 'do…', 'dra', 'dran', 'drar', 'dras', 'dre', 'dref', 'dres', 'drez', 'dri', 'dria', 'drid', 'drie', 'drio', 'drip', 'dro', 'dron', 'drop', 'dros', 'dru', 'drug', 'drugs', 'dry', 'drá', 'drán', 'drás', 'dré', 'drí', 'drón', 'ds!', 'ds3', 'dta', 'dto', 'du', 'dub', 'duc', 'due', 'duen', 'duer', 'dul', 'dum', 'dun', 'duo', 'dur', 'dus', 'duz', 'dwi', 'dy', 'dyba', 'dá', 'dán', 'dás', 'dé', 'dén', 'dés', 'dí', 'dín', 'dís', 'dó', 'dón', 'dú', 'e', 'e!', 'ec', 'ed', 'ef', 'eh', 'ei', 'eins', 'eiou', 'ek', 'el', 'em', 'en', 'ens', 'ep', 'er', 'ers', 'es', 'et', 'eu', 'ew', 'ex', 'ey', 'ez', 'f', 'fa', 'fac', 'fal', 'fan', 'fans', 'far', 'fas', 'fat', 'fau', 'faz', 'fe', 'fe!', 'fec', 'fei', 'fen', 'fer', 'fes', 'ff', 'fi', 'fia', 'fian', 'fiar', 'fic', 'fie', 'fiel', 'fien', 'fier', 'fies', 'fif', 'fig', 'fil', 'fin', 'fio', 'fir', 'fis', 'fit', 'fió', 'fl', 'fla', 'flan', 'flas', 'flau', 'fle', 'flec', 'fles', 'flex', 'fli', 'flic', 'flig', 'flip', 'flix', 'flo', 'floc', 'flor', 'flow', 'flows', 'flu', 'fluen', 'flui', 'fluir', 'fly', 'fms', 'fn', 'fo', 'fol', 'fom', 'fon', 'for', 'fos', 'four', 'fra', 'frac', 'fran', 'frar', 'fras', 'frau', 'fre', 'fred', 'fren', 'fres', 'freud', 'frez', 'fri', 'fria', 'frie', 'frien', 'frio', 'frir', 'frió', 'fro', 'from', 'fron', 'fru', 'frus', 'frá', 'frán', 'fré', 'frí', 'frú', 'fss', 'ft', 'fu', 'fuc', 'fue', 'fuen', 'fuer', 'fui', 'fuis', 'ful', 'fun', 'fur', 'fué', 'fuí', 'fy', 'fá', 'fé', 'fí', 'fó', 'fón', 'fór', 'fós', 'fú', 'fút', 'g', 'ga', 'ga!', 'gab', 'gac', 'gai', 'gain', 'gal', 'gam', 'gan', 'gans', 'gar', 'gas', 'gat', 'gauc', 'gaz', 'ge', 'gel', 'gen', 'ger', 'ges', 'get', 'gg', 'ggan', 'gh', 'ghet', 'ghi', 'gho', 'gi', 'gia', 'gian', 'giar', 'gias', 'gie', 'gien', 'gil', 'gim', 'gin', 'gins', 'gio', 'gion', 'gir', 'gis', 'gién', 'gió', 'gión', 'gla', 'glar', 'glas', 'gle', 'gles', 'gli', 'glit', 'glo', 'glos', 'glow', 'glá', 'glés', 'glí', 'glón', 'go', 'go!', 'go;', 'god', 'goi', 'gol', 'gon', 'gor', 'gos', 'got', 'gps', 'gra', 'graf', 'gram', 'gran', 'grar', 'gras', 'gre', 'gres', 'grey', 'gri', 'grie', 'grin', 'gris', 'gro', 'gros', 'grou', 'grr', 'grrgrr', 'gru', 'grue', 'gry', 'grá', 'gré', 'grí', 'gró', 'gta', 'gto', 'gu', 'gua', 'guac', 'gual', 'guan', 'guar', 'guas', 'guay', 'guc', 'gue', 'guel', 'guen', 'gues', 'guet', 'gui', 'guia', 'guian', 'guiar', 'guie', 'guien', 'guin', 'guir', 'gun', 'guns', 'guo', 'gus', 'guz', 'gué', 'guí', 'guís', 'gy', 'gyal', 'gz', 'gá', 'gáis', 'gán', 'gáns', 'gár', 'gás', 'gé', 'gí', 'gó', 'gón', 'gún', 'güen', 'güey', 'güi', 'h', 'h!', 'h;', 'ha', 'hac', 'hah', 'hai', 'hal', 'ham', 'han', 'hap', 'har', 'has', 'hat', 'hats', 'hay', 'haz', 'hba', 'hd', 'he', 'hec', 'heh', 'hel', 'hem', 'hen', 'her', 'hers', 'hes', 'hey', 'hh', 'hi', 'hie', 'hier', 'hil', 'him', 'hin', 'hion', 'hip', 'hips', 'his', 'hit', 'hits', 'hli', 'hm', 'hmm', 'hn', 'ho', 'ho!', 'hoh', 'hol', 'hom', 'hon', 'hop', 'hor', 'hos', 'hot', 'hou', 'hous', 'how', 'hoy', 'hoz', 'hsi', 'ht', 'hter', 'hu', 'hua', 'huah', 'hue', 'huel', 'huer', 'hug', 'huh', 'huir', 'hum', 'hun', 'huoh', 'hus', 'huás', 'hy', 'hys', 'há', 'hán', 'ház', 'hé', 'héc', 'hí', 'hín', 'hís', 'hó', 'hón', 'hú', 'i', 'iah', 'ie', 'if', 'ig', 'im', 'in', 'ins', 'io', 'ip', 'ir', 'is', 'it', 'its', 'iz', 'i…', 'j', 'ja', 'jac', 'jag', 'jah', 'jai', 'jal', 'jam', 'jan', 'jar', 'jas', 'jau', 'jay', 'je', 'jeh', 'jei', 'jem', 'jen', 'jer', 'jes', 'jet', 'jhon', 'ji', 'jim', 'jin', 'jis', 'jno', 'jo', 'joi', 'jon', 'jor', 'jos', 'ju', 'jua', 'juan', 'jue', 'juez', 'juh', 'jui', 'jun', 'jus', 'juz', 'jà', 'já', 'ján', 'jás', 'jé', 'jér', 'jí', 'jó', 'jón', 'jú', 'k', 'k!', 'k;', 'ka', 'kah', 'kai', 'kan', 'kap', 'kar', 'kas', 'kay', 'kbus', 'ke', 'ked', 'keiou', 'ken', 'ker', 'kers', 'kes', 'ket', 'kets', 'keup', 'key', 'kham', 'khe', 'ki', 'ki!', 'kia', 'kiar', 'kid', 'kids', 'kie', 'kien', 'kim', 'kin', 'kios', 'kis', 'kit', 'kié', 'kiés', 'klan', 'klei', 'klein', 'klin', 'kne', 'knig', 'kno', 'knoc', 'know', 'knows', 'ko', 'ko!', 'kom', 'kon', 'kos', 'kout', 'kpu', 'kpú', 'krip', 'krus', 'kson', 'ku', 'kwo', 'ky', 'l', 'l!', 'l;', 'la', 'la!', 'la;', 'lab', 'lai', 'lam', 'lan', 'lar', 'las', 'lau', 'lax', 'laxy', 'lay', 'lco', 'ld', 'le', 'le;', 'lec', 'leg', 'lei', 'len', 'ler', 'lers', 'les', 'let', 'lets', 'lex', 'ley', 'leys', 'lez', 'lf', 'lfil', 'lfs', 'li', 'lia', 'lian', 'lias', 'lic', 'lie', 'lien', 'lig', 'lil', 'lim', 'lin', 'lio', 'lion', 'lios', 'lip', 'lir', 'lis', 'lit', 'lium', 'liz', 'lién', 'lió', 'lk', 'll', 'll;', 'lla', 'llah', 'llan', 'llar', 'llas', 'lle', 'llen', 'ller', 'lles', 'lley', 'lli', 'lliam', 'llie', 'llies', 'llin', 'llion', 'llions', 'llla', 'llo', 'llo!', 'llom', 'llon', 'llos', 'llow', 'llpa', 'lls', 'llu', 'llue', 'lly', 'llá', 'llán', 'llé', 'llí', 'llín', 'lló', 'llón', 'lm', 'lo', 'lo!', 'lob', 'loc', 'loj', 'lom', 'lon', 'lor', 'los', 'lot', 'lou', 'louis', 'low', 'loz', 'lre', 'lt', 'lu', 'luc', 'lud', 'lue', 'lui', 'luian', 'luis', 'lum', 'lun', 'luz', 'ly', 'lyri', 'lá', 'lám', 'lán', 'lás', 'lé', 'léc', 'lí', 'lím', 'lín', 'lís', 'ló', 'lón', 'lú', 'l…', 'm', 'm!', 'm;', 'ma', 'ma;', 'mac', 'mag', 'mai', 'main', 'mak', 'mal', 'mam', 'man', 'mar', 'mas', 'mau', 'max', 'may', 'mb', 'mbar', 'mc', 'mcdo', 'mcs', 'md', 'mdb', 'me', 'mec', 'mek', 'mel', 'mem', 'men', 'mens', 'mer', 'mes', 'met', 'mew', 'mex', 'mey', 'mez', 'me\\u2005', 'mhm', 'mi', 'mia', 'mias', 'miau', 'mic', 'mie', 'miel', 'mien', 'mier', 'mies', 'mil', 'min', 'mio', 'mios', 'mir', 'mis', 'mit', 'mix', 'mix!', 'mién', 'mió', 'mi—', 'mks', 'mm', 'mmh', 'mmm', 'mn', 'mo', 'mo!', 'mob', 'moc', 'moi', 'mol', 'mon', 'mons', 'mor', 'mos', 'mot', 'mou', 'mp', 'mr', 'mru', 'mrum', 'mtv', 'mu', 'mua', 'muc', 'mue', 'muek', 'muer', 'mues', 'mul', 'mun', 'mur', 'mus', 'muy', 'mué', 'mvp', 'my', 'myke', 'mys', 'má', 'mán', 'már', 'más', 'mé', 'mén', 'mí', 'mí;', 'mís', 'mó', 'món', 'mú', 'mún', 'n', 'n!', 'n;', 'na', 'na!', 'na;', 'nac', 'naf', 'nah', 'nai', 'naik', 'nal', 'nan', 'nar', 'nas', 'nat', 'nau', 'nax', 'naz', 'nc', 'nd', 'nds', 'ne', 'ne;', 'nec', 'neg', 'nei', 'neil', 'nel', 'nem', 'nen', 'nep', 'ner', 'nes', 'net', 'neu', 'new', 'nex', 'next', 'ney', 'nez', 'ng', 'ngs', 'ni', 'nia', 'nial', 'nias', 'nic', 'nie', 'niel', 'nien', 'nies', 'nig', 'nil', 'nin', 'nio', 'nion', 'nios', 'nir', 'nis', 'nix', 'nién', 'nión', 'ni‚', 'nk', 'nks', 'nla', 'nle', 'nlo', 'nly', 'nn', 'no', 'no!', 'no;', 'noc', 'noi', 'noia', 'nom', 'non', 'nor', 'nos', 'not', 'now', 'nox', 'noz', 'no\\u2005', 'no…', 'nra', 'nre', 'nri', 'nrie', 'nrien', 'nrio', 'nrió', 'nro', 'nrou', 'nrro', 'nrí', 'nt', 'nto', 'nts', 'nu', 'nuar', 'nue', 'nuel', 'nues', 'nui', 'num', 'nun', 'nus', 'nx', 'ny', 'nyno', 'nz', 'ná', 'nán', 'nás', 'né', 'néc', 'nés', 'ní', 'nín', 'nís', 'nó', 'nón', 'nú', 'o', 'o!', 'ob', 'obs', 'oc', 'od', 'of', 'og', 'oh', 'oi', 'oie', 'ois', 'ok', 'oks', 'ol', 'om', 'on', 'op', 'ops', 'or', 'os', 'ot', 'ou', 'oui', 'ouo', 'our', 'out', 'ov', 'ow', 'ox', 'oy', 'p', 'p!', 'p;', 'pa', 'pa!', 'pac', 'pag', 'pai', 'pain', 'pais', 'pal', 'pam', 'pan', 'pans', 'pap', 'paq', 'par', 'pas', 'pau', 'paul', 'pay', 'paz', 'pa…', 'pe', 'pe!', 'pec', 'pei', 'pel', 'pen', 'pep', 'per', 'pers', 'pes', 'pew', 'pez', 'ph', 'pher', 'phi', 'pho', 'pi', 'pia', 'pian', 'piar', 'pias', 'pic', 'pie', 'pie!', 'piel', 'pien', 'pier', 'pies', 'pif', 'pil', 'pim', 'pin', 'pio', 'pion', 'pions', 'pios', 'pip', 'pir', 'pis', 'pit', 'piu', 'piz', 'piár', 'pié', 'pién', 'piér', 'pió', 'pión', 'pla', 'plan', 'plas', 'plau', 'plax', 'play', 'plays', 'plaz', 'ple', 'plen', 'ples', 'pli', 'plien', 'plik', 'plir', 'plis', 'plo', 'ploc', 'plu', 'plug', 'plus', 'plá', 'plás', 'plé', 'plí', 'po', 'po!', 'po;', 'poc', 'pol', 'pom', 'pon', 'pop', 'por', 'pors', 'pos', 'pot', 'pow', 'pp', 'ppin', 'pprru', 'pq', 'pr', 'pra', 'prac', 'pran', 'prar', 'pras', 'pre', 'pren', 'pres', 'pret', 'pri', 'pric', 'prie', 'prim', 'prin', 'prio', 'prit', 'prié', 'pro', 'pron', 'pros', 'prr', 'prr;', 'prra', 'prra!', 'prrr', 'prrra', 'prrrum', 'prruf', 'prue', 'prác', 'pré', 'prén', 'prín', 'pró', 'psh', 'pshh', 'psi', 'psy', 'pu', 'puc', 'pue', 'puel', 'puen', 'puer', 'pues', 'pul', 'pum', 'pun', 'pur', 'pus', 'put', 'pué', 'pués', 'py', 'pá', 'pán', 'pás', 'pé', 'pés', 'pí', 'pó', 'pón', 'pór', 'pós', 'pú', 'púr', 'q', 'qe', 'qua', 'quad', 'que', 'quel', 'quen', 'ques', 'quez', 'que\\u2005', 'qui', 'quia', 'quie', 'quien', 'quier', 'quii', 'quik', 'quil', 'quim', 'quin', 'quios', 'quis', 'quié', 'quién', 'qué', 'quél', 'qués', 'quí', 'quí!', 'r', 'r!', 'r15', 'r;', 'ra', 'ra!', 'ra;', 'rac', 'rai', 'rain', 'ral', 'ram', 'ran', 'rap', 'raps', 'rar', 'ras', 'rat', 'rats', 'rau', 'rax', 'raxxe', 'raz', 'rc', 'rcar', 'rcer', 'rci', 'rcir', 'rd', 'rdar', 'rde', 'rder', 'rdo', 'rds', 're', 'rec', 'red', 'reg', 'rei', 'ren', 'rer', 'res', 'ret', 'reu', 'rey', 'rez', 'rgar', 'ri', 'ria', 'riac', 'rial', 'rian', 'riar', 'rias', 'ric', 'rie', 'riel', 'rien', 'ries', 'rig', 'ril', 'rin', 'rio', 'rior', 'rios', 'rious', 'rio…', 'rip', 'rir', 'ris', 'rit', 'rix', 'riz', 'rién', 'rió', 'rk', 'rker', 'rl', 'rla', 'rlan', 'rlar', 'rlas', 'rld', 'rle', 'rles', 'rley', 'rli', 'rlie', 'rlo', 'rlos', 'rls', 'rly', 'rm', 'rmar', 'rmi', 'rmir', 'rn', 'rnar', 'rner', 'ro', 'ro!', 'ro;', 'roc', 'rok', 'rol', 'rom', 'ron', 'ror', 'ros', 'rot', 'rou', 'rpar', 'rra', 'rrac', 'rrai', 'rran', 'rrap', 'rrar', 'rras', 'rre', 'rrec', 'rren', 'rrer', 'rres', 'rri', 'rrian', 'rrien', 'rries', 'rril', 'rrin', 'rrio', 'rrior', 'rriors', 'rrios', 'rrió', 'rrión', 'rro', 'rroc', 'rrom', 'rron', 'rror', 'rros', 'rroz', 'rrra', 'rru', 'rruc', 'rrui', 'rrum', 'rrup', 'rry', 'rrá', 'rré', 'rrí', 'rró', 'rrón', 'rsa', 'rsar', 'rt', 'rta', 'rtar', 'rten', 'rti', 'rtier', 'rtir', 'ru', 'rua', 'rue', 'rui', 'rum', 'run', 'rus', 'rvir', 'ry', 'rzar', 'rá', 'rác', 'rái', 'rán', 'rás', 'ré', 'réis', 'rés', 'rí', 'rín', 'rís', 'rít', 'ró', 'róm', 'rón', 'rú', 'r…', 's', 's!', 's;', 'sa', 'sa;', 'sad', 'sai', 'sal', 'sam', 'san', 'sap', 'sar', 'sas', 'sau', 'sauer', 'saw', 'say', 'sc', 'scar', 'sco', 'scre', 'sd', 'se', 'sec', 'sed', 'seh', 'sei', 'seis', 'sel', 'sem', 'sen', 'sep', 'ser', 'ses', 'set', 'sex', 'sexy', 'sh', 'sha', 'shaw', 'she', 'shh', 'shi', 'shit', 'sho', 'shop', 'shor', 'shot', 'shou', 'shout', 'show', 'shows', 'shut', 'si', 'sia', 'sian', 'sias', 'sic', 'sie', 'siem', 'sien', 'sier', 'sies', 'sig', 'sil', 'sim', 'sin', 'sio', 'sion', 'sip', 'sir', 'sis', 'sit', 'six', 'sién', 'sión', 'sk', 'ska', 'ske', 'ski', 'skin', 'skrrt', 'skrt', 'sky', 'slam', 'sle', 'slow', 'sma', 'sme', 'smi', 'smo', 'snap', 'sne', 'sni', 'snip', 'snit', 'sno', 'so', 'soi', 'sol', 'som', 'son', 'sor', 'sos', 'sou', 'soul', 'sour', 'sout', 'soy', 'spa', 'spain', 'spe', 'spi', 'spin', 'spit', 'spo', 'spot', 'spray', 'spri', 'squad', 'ss', 'ssa', 'ssan', 'sse', 'ssed', 'ssi', 'ssiah', 'ssion', 'ssj', 'sso', 'sson', 'sss', 'ssy', 'ssys', 'st', 'sta', 'star', 'ste', 'sti', 'stic', 'stig', 'sto', 'stop', 'stra', 'straig', 'stre', 'stri', 'strip', 'stro', 'stu', 'stua', 'su', 'sua', 'sual', 'sub', 'suc', 'sue', 'suel', 'suer', 'sui', 'sul', 'sun', 'sup', 'sur', 'sus', 'suél', 'swag', 'swe', 'swi', 'swim', 'sx', 'sy', 'sá', 'sál', 'sán', 'sár', 'sás', 'sé', 'séis', 'sí', 'sí;', 'sín', 'só', 'són', 'sú', 'sús', 's…', 't', 't!', 'ta', 'ta!', 'ta;', 'tac', 'tad', 'tag', 'tags', 'tai', 'tal', 'tam', 'tan', 'tar', 'tas', 'tat', 'tau', 'tc', 'tci', 'te', 'te!', 'te;', 'tec', 'ted', 'tein', 'tek', 'tel', 'tem', 'ten', 'ter', 'ters', 'tes', 'tex', 'text', 'th', 'tha', 'that', 'thats', 'the', 'them', 'they', 'thi', 'this', 'tho', 'thou', 'thre', 'throu', 'throw', 'thug', 'thugs', 'ti', 'tia', 'tial', 'tian', 'tiar', 'tias', 'tic', 'tie', 'tiem', 'tien', 'tier', 'ties', 'tii', 'tik', 'til', 'tim', 'tin', 'tio', 'tion', 'tios', 'tir', 'tis', 'tiu', 'tién', 'tió', 'tión', 'tlas', 'tle', 'tli', 'tni', 'to', 'to!', 'to;', 'toc', 'toi', 'tol', 'tom', 'ton', 'top', 'tor', 'tos', 'tou', 'touc', 'tour', 'tout', 'toy', 'toys', 'tr', 'tr1', 'tra', 'trac', 'trai', 'tral', 'tram', 'tran', 'trans', 'trap', 'trar', 'tras', 'trau', 'traz', 'tre', 'trec', 'trein', 'tren', 'tres', 'tri', 'tria', 'tric', 'trig', 'tril', 'trin', 'trip', 'tris', 'triun', 'triz', 'tro', 'troc', 'troit', 'trol', 'trom', 'tron', 'tros', 'troy', 'tro…', 'tru', 'truc', 'true', 'truir', 'truo', 'truí', 'try', 'trá', 'trái', 'trás', 'tré', 'trés', 'trí', 'tró', 'trón', 'ts;', 'tss', 'tt', 'tu', 'tua', 'tual', 'tuan', 'tuar', 'tuc', 'tud', 'tuer', 'tum', 'tun', 'tur', 'tus', 'tut', 'tué', 'tv', 'twe', 'twei', 'twen', 'twer', 'twin', 'two', 'ty', 'tyle', 'tyro', 'tys', 'tá', 'tác', 'tán', 'tár', 'tás', 'té', 'téc', 'tén', 'tés', 'tí', 'tím', 'tín', 'tís', 'tó', 'tón', 'tú', 'túu', 'u', 'ua', 'uah', 'ud', 'uds', 'ueh', 'uf', 'uh', 'uiui', 'ul', 'um', 'un', 'uns', 'uo', 'uoh', 'up', 'ur', 'us', 'uu', 'uuh', 'uuu', 'uuuh', 'uy', 'v', 'va', 'vac', 'vai', 'val', 'vam', 'van', 'var', 'vas', 'vck', 've', 'vec', 'vein', 'vel', 'ven', 'ver', 'ves', 'vez', 'vi', 'via', 'vial', 'viar', 'vias', 'vic', 'vid', 'vie', 'viem', 'vien', 'vier', 'views', 'vil', 'vin', 'vio', 'vion', 'vios', 'vip', 'vir', 'vis', 'vién', 'vió', 'vión', 'vo', 'voi', 'vol', 'vor', 'vos', 'voy', 'voz', 'vu', 'vue', 'vuel', 'vuit', 'vul', 'vuél', 'vvs', 'vy', 'vá', 'ván', 'vé', 'véc', 'vén', 'vés', 'ví', 'víc', 'vís', 'vó', 'vеn', 'w', 'w!', 'w;', 'wa', 'wac', 'wah', 'wai', 'wait', 'wal', 'wan', 'war', 'wars', 'was', 'wat', 'wax', 'way', 'ways', 'we', 'web', 'weh', 'wel', 'wer', 'wers', 'wey', 'what', 'whe', 'when', 'whi', 'whip', 'whis', 'whit', 'who', 'why', 'wi', 'win', 'wis', 'wiu', 'wn', 'wns', 'wo', 'wo!', 'woh', 'wom', 'won', 'wos', 'wouh', 'wouo', 'wow', 'wri', 'wu', 'wuh', 'wá', 'wán', 'x', 'x2', 'xa', 'xac', 'xan', 'xas', 'xd', 'xe', 'xi', 'xia', 'xian', 'xin', 'xio', 'xir', 'xis', 'xit', 'xión', 'xo', 'xua', 'xí', 'xó', 'y', 'y!', 'y;', 'ya', 'ya!', 'yah', 'yam', 'yan', 'yar', 'yas', 'yay', 'yd', 'yday', 'yde', 'ye', 'ye!', 'yec', 'yeh', 'yei', 'yen', 'yer', 'yers', 'yes', 'yey', 'yfa', 'yfour', 'yi', 'yih', 'yin', 'yle', 'yler', 'ylers', 'yles', 'yli', 'yn', 'ynn', 'yno', 'yo', 'yo!', 'yoh', 'yon', 'yor', 'yos', 'you', 'your', 'ysy', 'yu', 'yum', 'yuo', 'ywo', 'yy', 'yá', 'yén', 'yó', 'y…', 'z', 'z!', 'za', 'zag', 'zal', 'zam', 'zan', 'zar', 'zas', 'ze', 'zen', 'zep', 'zer', 'zes', 'zi', 'zig', 'zion', 'zip', 'zis', 'zlo', 'zlu', 'zo', 'zom', 'zon', 'zos', 'zu', 'zu!', 'zue', 'zul', 'zum', 'zy', 'zys', 'zz', 'zá', 'zán', 'zás', 'zé', 'zó', 'zón', 'zú', '¡!', '¡a', '¡ah', '¡ay', '¡be', '¡bo', '¡bri', '¡can', '¡caz', '¡ce', '¡crac', '¡cuan', '¡dro', '¡du', '¡dí', '¡e', '¡es', '¡ja!', '¡no', '¡no!', '¡o', '¡pe', '¡prra!', '¡que', '¡qué', '¡to', '¡u', '¡wo', '¡wou', '¡wu', '¡wuh', '¡ya', '¡yay', '¡ye', '¿', 'á', 'ál', 'ám', 'án', 'ár', 'ás', 'é', 'éc', 'él', 'és', 'í', 'ín', 'ír', 'ís', 'íz', 'ña', 'ñal', 'ñan', 'ñar', 'ñas', 'ñe', 'ñen', 'ñes', 'ñi', 'ñia', 'ñias', 'ñir', 'ño', 'ño;', 'ñol', 'ñon', 'ñor', 'ños', 'ñán', 'ñé', 'ñí', 'ñó', 'ñón', 'ó', 'ón', 'ór', 'ú', 'úd', 'úl', 'ún', 'üe', 'еn', 'еro', '\\u2005a', '\\u2005el', '\\u2005en', '\\u2005es', '\\u2005la', '\\u2005la\\u2005', '—', '‘no', '‘toy', '‘tá', '…!']\n",
            "Unique words after ignoring: 2687\n",
            "Ignored sequences: 21322\n",
            "Remaining sequences: 1329840\n",
            "Shuffling sentences\n",
            "Size of training set = 1303243\n",
            "Size of test set = 26597\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faPI2GhGd93t"
      },
      "source": [
        "Creacion de modelo y entrenamiento extraido de https://github.com/enriqueav/lstm_lyrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Et7GeMITcqcj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b24b641b-3204-4f2a-cc85-1a17997fc460"
      },
      "source": [
        "\"\"\"\n",
        "Example script to train a network to generate text with the style of a given corpus\n",
        "--By word--\n",
        "\n",
        "It is recommended to run tweights_10_10_v1(of)his script on GPU, as recurrent\n",
        "networks are quite computationally intensive.\n",
        "\n",
        "Based on\n",
        "https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
        "\n",
        "20 epochs should be enough to get decent results.\n",
        "Uses data generator to avoid loading all the test set into memory.\n",
        "Saves the weights and model every epoch.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import print_function\n",
        "from keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional\n",
        "import numpy as np\n",
        "import sys\n",
        "import io\n",
        "import os\n",
        "import json\n",
        "from google.colab import files\n",
        "\n",
        "BATCH_SIZE = 512\n",
        "\n",
        "\n",
        "# Data generator for fit and evaluate\n",
        "def generator(sentence_list, next_word_list, batch_size):\n",
        "    index = 0\n",
        "    while True:\n",
        "        x = np.zeros((batch_size, SEQUENCE_LEN, len(words)), dtype=np.bool)\n",
        "        y = np.zeros((batch_size, len(words)), dtype=np.bool)\n",
        "        for i in range(batch_size):\n",
        "            for t, w in enumerate(sentence_list[index % len(sentence_list)]):\n",
        "                x[i, t, word_indices[w]] = 1\n",
        "            y[i, word_indices[next_word_list[index % len(sentence_list)]]] = 1\n",
        "            index = index + 1\n",
        "        yield x, y\n",
        "\n",
        "\n",
        "def get_model(dropout=0.2):\n",
        "    print('Build model...')\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(256), input_shape=(SEQUENCE_LEN, len(words))))\n",
        "    if dropout > 0:\n",
        "        model.add(Dropout(dropout))\n",
        "    model.add(Dense(len(words)))\n",
        "    model.add(Activation('softmax'))\n",
        "    return model\n",
        "\n",
        "\n",
        "# Functions from keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "def on_epoch_end(epoch, logs):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    examples_file.write('\\n----- Generating text after Epoch: %d\\n' % epoch)\n",
        "\n",
        "    # Randomly pick a seed sequence\n",
        "    seed_index = np.random.randint(len(sentences+sentences_test))\n",
        "    seed = (sentences+sentences_test)[seed_index]\n",
        "\n",
        "    for diversity in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
        "        sentence = seed\n",
        "        examples_file.write('----- Diversity:' + str(diversity) + '\\n')\n",
        "        examples_file.write('----- Generating with seed:\\n\"' + ' '.join(sentence) + '\"\\n')\n",
        "        examples_file.write(' '.join(sentence))\n",
        "\n",
        "        for i in range(50):\n",
        "            x_pred = np.zeros((1, SEQUENCE_LEN, len(words)))\n",
        "            for t, word in enumerate(sentence):\n",
        "                x_pred[0, t, word_indices[word]] = 1.\n",
        "\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_word = indices_word[next_index]\n",
        "\n",
        "            sentence = sentence[1:]\n",
        "            sentence.append(next_word)\n",
        "\n",
        "            examples_file.write(\" \"+next_word)\n",
        "        examples_file.write('\\n')\n",
        "    examples_file.write('='*80 + '\\n')\n",
        "    examples_file.flush()\n",
        "\n",
        "\n",
        "model = get_model(0.4)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "file_path = \"./checkpoints/LSTM_LYRICS-epoch{epoch:03d}-words%d-sequence%d-minfreq%d-\" \\\n",
        "            \"loss{loss:.4f}-acc{accuracy:.4f}-val_loss{val_loss:.4f}-val_acc{val_accuracy:.4f}\" % \\\n",
        "            (len(words), SEQUENCE_LEN, MIN_WORD_FREQUENCY)\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath=file_path, monitor='val_accuracy', save_best_only=True)\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "early_stopping = EarlyStopping(monitor='accuracy', patience=5)\n",
        "callbacks_list = [checkpoint, print_callback, early_stopping]\n",
        "\n",
        "examples_file = open(examples, \"w\")\n",
        "\n",
        "if 'weights_s.h5' in os.listdir():\n",
        "  print(\"Weights loaded\")\n",
        "  model.load_weights('weights_s.h5')\n",
        "\n",
        "history = model.fit(generator(sentences, next_words, BATCH_SIZE),\n",
        "                    steps_per_epoch=int(len(sentences)/BATCH_SIZE) + 1,\n",
        "                    epochs=10,\n",
        "                    #callbacks=callbacks_list,\n",
        "                    validation_data=generator(sentences_test, next_words_test, BATCH_SIZE),\n",
        "                    validation_steps=int(len(sentences_test)/BATCH_SIZE) + 1)\n",
        "\n",
        "model.save_weights('weights_s.h5')\n",
        "\n",
        "with io.open('history_s.json', 'w') as history_file:\n",
        "    json.dump(history.history, history_file)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build model...\n",
            "Weights loaded\n",
            "Epoch 1/10\n",
            "2546/2546 [==============================] - 153s 59ms/step - loss: 0.8739 - accuracy: 0.7821 - val_loss: 2.5956 - val_accuracy: 0.6082\n",
            "Epoch 2/10\n",
            "2546/2546 [==============================] - 155s 61ms/step - loss: 0.8706 - accuracy: 0.7829 - val_loss: 2.5888 - val_accuracy: 0.6096\n",
            "Epoch 3/10\n",
            "2546/2546 [==============================] - 155s 61ms/step - loss: 0.8679 - accuracy: 0.7837 - val_loss: 2.5973 - val_accuracy: 0.6115\n",
            "Epoch 4/10\n",
            "2546/2546 [==============================] - 154s 61ms/step - loss: 0.8634 - accuracy: 0.7846 - val_loss: 2.6030 - val_accuracy: 0.6105\n",
            "Epoch 5/10\n",
            "2546/2546 [==============================] - 154s 61ms/step - loss: 0.8591 - accuracy: 0.7856 - val_loss: 2.6039 - val_accuracy: 0.6110\n",
            "Epoch 6/10\n",
            "2546/2546 [==============================] - 154s 60ms/step - loss: 0.8576 - accuracy: 0.7863 - val_loss: 2.5973 - val_accuracy: 0.6108\n",
            "Epoch 7/10\n",
            "2546/2546 [==============================] - 154s 61ms/step - loss: 0.8552 - accuracy: 0.7859 - val_loss: 2.6097 - val_accuracy: 0.6104\n",
            "Epoch 8/10\n",
            "2546/2546 [==============================] - 154s 60ms/step - loss: 0.8501 - accuracy: 0.7871 - val_loss: 2.6188 - val_accuracy: 0.6098\n",
            "Epoch 9/10\n",
            "2546/2546 [==============================] - 154s 60ms/step - loss: 0.8489 - accuracy: 0.7870 - val_loss: 2.6314 - val_accuracy: 0.6121\n",
            "Epoch 10/10\n",
            "2546/2546 [==============================] - 154s 61ms/step - loss: 0.8440 - accuracy: 0.7885 - val_loss: 2.6204 - val_accuracy: 0.6108\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9gXFhwhtiaF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "2da1be35-2e5d-4905-b7a4-eda934f10b56"
      },
      "source": [
        "model.save_weights('weights_s.h5')\n",
        "files.download('weights_s.h5')\n",
        "files.download('history_s.json')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_a79b9d8e-684a-463c-a51f-4f9012494608\", \"weights_s.h5\", 29650460)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_bd1b274f-9c52-4d95-8fa8-67a40fdb3d35\", \"history_s.json\", 841)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xiVw9l_eGK1"
      },
      "source": [
        "Generacion de texto con el modelo anterior"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjHYZHmrba7B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dac4d3f8-e28a-40f3-8797-3cbda0993194"
      },
      "source": [
        "\"\"\"\n",
        "Script to generate text from an already trained network (with lstm_train.py)\n",
        "--By word--\n",
        "\n",
        "It is necessary to at least provide the trained model and the vocabulary file\n",
        "(generated also by lstm_train.py).\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "import re\n",
        "from keras.models import load_model\n",
        "\n",
        "import collections\n",
        "\n",
        "\n",
        "def validate_seed(vocabulary, seed):\n",
        "    \"\"\"Validate that all the words in the seed are part of the vocabulary\"\"\"\n",
        "    print(\"\\nValidating that all the words in the seed are part of the vocabulary: \")\n",
        "    seed_words = seed.split(\" \")\n",
        "    valid = True\n",
        "    for w in seed_words:\n",
        "        print(w, end=\"\")\n",
        "        if w in vocabulary:\n",
        "            print(\" ✓ in vocabulary\")\n",
        "        else:\n",
        "            print(\" ✗ NOT in vocabulary\")\n",
        "            valid = False\n",
        "    return valid\n",
        "\n",
        "\n",
        "# Functions from keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "def rima(old_syl, next_syl):        \n",
        "    vowels = ['a','e','i','o','u','á', 'é', 'í', 'ó', 'ú', 'y']\n",
        "    if old_syl==next_syl:\n",
        "        return 0.6\n",
        "    old_strip=\"\".join(filter(lambda c: c in vowels, old_syl))\n",
        "    next_strip=\"\".join(filter(lambda c: c in vowels, next_syl))\n",
        "    return 0.5 if old_strip==next_strip else 0\n",
        "\n",
        "\n",
        "def rhyme_score(preds, previous_syllables):\n",
        "    #Previous syllables deberia ser la ultima silaba, o las dos ultimas silabas concatenadas    \n",
        "    for index in range(len(preds)):\n",
        "        if(preds[index]) > 0.6: #Modificar a verdaderas candidatas a sílaba\n",
        "            preds[index] += (1.0 - preds[index])*rima(previous_syllables,indices_word[index]) \n",
        "    return preds\n",
        "\n",
        "\n",
        "def generate_text(model, indices_word, word_indices, seed,\n",
        "                  sequence_length, diversity, quantity):\n",
        "    \"\"\"\n",
        "    Similar to lstm_train::on_epoch_end\n",
        "    Used to generate text using a trained model\n",
        "\n",
        "    :param model: the trained Keras model (with model.load)\n",
        "    :param indices_word: a dictionary pointing to the words\n",
        "    :param seed: a string to be used as seed (already validated and padded)\n",
        "    :param sequence_length: how many words are given to the model to generate\n",
        "    :param diversity: is the \"temperature\" of the sample function (usually between 0.1 and 2)\n",
        "    :param quantity: quantity of words to generate\n",
        "    :return: Nothing, for now only writes the text to console\n",
        "    \"\"\"\n",
        "    sentence = seed.split(\" \")\n",
        "    print(\"----- Generating text\")\n",
        "    print('----- Diversity:' + str(diversity))\n",
        "    print('----- Generating with seed:\\n\"' + seed)\n",
        "\n",
        "    print()\n",
        "    print(seed)\n",
        "\n",
        "    prev_preds = []\n",
        "    text = []\n",
        "    last_syllables = []\n",
        "    elegidas = []\n",
        "    elegidas_text = []\n",
        "    for i in range(quantity):\n",
        "        x_pred = np.zeros((1, sequence_length, len(vocabulary)))\n",
        "        for t, word in enumerate(sentence):\n",
        "            x_pred[0, t, word_indices[word]] = 1.\n",
        "\n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        # preds tiene arreglo resultado de softmax\n",
        "\n",
        "        next_index = sample(preds, diversity)\n",
        "        next_word = indices_word[next_index]\n",
        "\n",
        "        if next_word == '\\n' and len(text) > 0:\n",
        "          # redefinir sentence[-1] usando prev_preds (valor de la prediccion anterior)\n",
        "          \n",
        "          if (text[-1] != '\\n'):\n",
        "            silaba_reemplazada = text[-1]\n",
        "            if last_syllables != [] and (text[-1]!='\\\\' or text[-1]!='\\n'):              \n",
        "              changed_prev_index = sample(rhyme_score(prev_preds, last_syllables[-1][-1]))\n",
        "              new_word = indices_word[changed_prev_index]\n",
        "              if new_word != silaba_reemplazada:\n",
        "                sentence[-1] = new_word\n",
        "                new_word = new_word.upper()\n",
        "                text[-1] = new_word\n",
        "                elegidas_text.append((new_word,silaba_reemplazada))\n",
        "\n",
        "            last_syllables.append(list(filter(lambda x: (x!='\\\\'),sentence[-2:])))\n",
        "          else:\n",
        "            last_syllables = []\n",
        "\n",
        "        sentence = sentence[1:]\n",
        "        sentence.append(next_word)\n",
        "        text.append(next_word)\n",
        "        prev_word = next_word\n",
        "        prev_preds = preds\n",
        "\n",
        "    print(\"Pares de silabas reemplazadas, silabas originales:\")\n",
        "    print(elegidas_text)\n",
        "    print()\n",
        "    return text\n",
        "\n",
        "vocabulary_file = \"vocabulary.txt\"\n",
        "#model_file = args.network\n",
        "seed = \"voy a ser el que en el mi cro fo no de rap\"\n",
        "sequence_length = SEQUENCE_LEN\n",
        "diversity = 0.5\n",
        "quantity = 500\n",
        "\n",
        "#model = load_model(model_file)\n",
        "print(\"\\nSummary of the Network: \")\n",
        "model.summary()\n",
        "\n",
        "vocabulary = open(vocabulary_file, \"r\").readlines()\n",
        "# remove the \\n at the end of the word, except for the \\n word itself\n",
        "vocabulary = [re.sub(r'(\\S+)\\s+', r'\\1', w) for w in vocabulary]\n",
        "\n",
        "print([item for item, count in collections.Counter(vocabulary).items() if count > 1])\n",
        "\n",
        "#vocabulary = sorted(set(vocabulary))\n",
        "vocabulary = sorted(vocabulary)\n",
        "\n",
        "word_indices = dict((c, i) for i, c in enumerate(vocabulary))\n",
        "indices_word = dict((i, c) for i, c in enumerate(vocabulary))\n",
        "\n",
        "if validate_seed(vocabulary, seed):\n",
        "    print(\"\\nSeed is correct.\\n\")\n",
        "    # repeat the seed in case is not long enough, and take only the last elements\n",
        "    seed = \" \".join((((seed+\" \")*sequence_length)+seed).split(\" \")[-sequence_length:])\n",
        "    text = generate_text(\n",
        "        model, indices_word, word_indices, seed, sequence_length, diversity, quantity\n",
        "    )\n",
        "    for word in text: print(word if word != '\\\\' else ' ', end=\"\")\n",
        "else:\n",
        "    print('\\033[91mERROR: Please fix the seed string\\033[0m')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Summary of the Network: \n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_1 (Bidirection (None, 256)               1710080   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1541)              396037    \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 1541)              0         \n",
            "=================================================================\n",
            "Total params: 2,106,117\n",
            "Trainable params: 2,106,117\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[]\n",
            "\n",
            "Validating that all the words in the seed are part of the vocabulary: \n",
            "voy ✓ in vocabulary\n",
            "a ✓ in vocabulary\n",
            "ser ✓ in vocabulary\n",
            "el ✓ in vocabulary\n",
            "que ✓ in vocabulary\n",
            "en ✓ in vocabulary\n",
            "el ✓ in vocabulary\n",
            "mi ✓ in vocabulary\n",
            "cro ✓ in vocabulary\n",
            "fo ✓ in vocabulary\n",
            "no ✓ in vocabulary\n",
            "de ✓ in vocabulary\n",
            "rap ✓ in vocabulary\n",
            "\n",
            "Seed is correct.\n",
            "\n",
            "----- Generating text\n",
            "----- Diversity:0.5\n",
            "----- Generating with seed:\n",
            "\"el que en el mi cro fo no de rap\n",
            "\n",
            "el que en el mi cro fo no de rap\n",
            "Pares de silabas reemplazadas, silabas originales:\n",
            "[('LE', 'ma'), ('DAD', 'les'), ('CIA', 'te')]\n",
            "\n",
            " hoy es lo que la sabe , voy a conteces\n",
            "yo día te gano , soy un abrarios\n",
            "mi rap , hace nada la alLE\n",
            "\n",
            "porque en el anice , qué es como a mí me da pereza\n",
            "manos para arriba , que hoy ha venido la alteza\n",
            "voy a reventar a este rabioso\n",
            "porque pa ' ganarle no me cuesta , soy un oso\n",
            "pero hoy día queda , yo lo suelto amar a que  las manos\n",
            "\n",
            "y eso no me castiga\n",
            "este es verdad , yo soy el que te gana en el barrio\n",
            "yo ado a los de los que yo te contra\n",
            "te rapero que tengo a realiDAD\n",
            "tú eres un vivo y de nuevo a la tierra claro\n",
            "no me ahora porque me dice que me faltan huevos si te vai a nitica\n",
            "esa es la verdad , mejor que te este perferenCIA\n",
            "lo que dice me macripa que era en el freestyle hermano para ser rapero\n",
            "yo vengo a la caramente se acaba y con mi weones y no me sale ego en todos los dejo\n",
            "\n",
            "la verita con la pistaa todavía que me apanteo\n",
            "y cada"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}