{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Words.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMP3JkNCAwHm"
      },
      "source": [
        "\n",
        "Tomado de https://github.com/enriqueav/lstm_lyrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiWxMZNlBVt-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "753a7a78-f5bd-4086-aa8c-d83a254240cb"
      },
      "source": [
        "import os\n",
        "import io\n",
        "import codecs\n",
        "import re\n",
        "import numpy as np\n",
        "# Parameters: change to experiment different configurations\n",
        "SEQUENCE_LEN = 5\n",
        "MIN_WORD_FREQUENCY = 1\n",
        "STEP = 1\n",
        "\n",
        "def shuffle_and_split_training_set(sentences_original, next_original, percentage_test=2):\n",
        "    # shuffle at unison\n",
        "    print('Shuffling sentences')\n",
        "\n",
        "    tmp_sentences = []\n",
        "    tmp_next_word = []\n",
        "    for i in np.random.permutation(len(sentences_original)):\n",
        "        tmp_sentences.append(sentences_original[i])\n",
        "        tmp_next_word.append(next_original[i])\n",
        "\n",
        "    cut_index = int(len(sentences_original) * (1.-(percentage_test/100.)))\n",
        "    x_train, x_test = tmp_sentences[:cut_index], tmp_sentences[cut_index:]\n",
        "    y_train, y_test = tmp_next_word[:cut_index], tmp_next_word[cut_index:]\n",
        "\n",
        "    print(\"Size of training set = %d\" % len(x_train))\n",
        "    print(\"Size of test set = %d\" % len(y_test))\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "def print_vocabulary(words_file_path, words_set):\n",
        "    words_file = codecs.open(words_file_path, 'w', encoding='utf8')\n",
        "    for w in words_set:\n",
        "        if w != \"\\n\":\n",
        "            words_file.write(w+\"\\n\")\n",
        "        else:\n",
        "            words_file.write(w)\n",
        "    words_file.close()\n",
        "\n",
        "path = 'D:\\\\Documentos\\\\GitHub\\\\freestyle_generator\\\\data\\\\'\n",
        "corpus = path + \"corpus.txt\"\n",
        "examples = \"examples.txt\"\n",
        "vocabulary = \"vocabulary.txt\"\n",
        "\n",
        "with io.open(corpus, encoding='utf-8') as f:\n",
        "    text = f.read().lower().replace('\\n', ' \\n ')\n",
        "    text = text.replace('\\xa0', '')\n",
        "    text = re.sub('[.,:?¿¡!-]+','',text)\n",
        "print('Corpus length in characters:', len(text))\n",
        "\n",
        "text_in_words = [w for w in text.split(' ') if w.strip() != '' or w == '\\n']\n",
        "print('Corpus length in words:', len(text_in_words))\n",
        "\n",
        "# Calculate word frequency\n",
        "word_freq = {}\n",
        "for word in text_in_words:\n",
        "    word_freq[word] = word_freq.get(word, 0) + 1\n",
        "\n",
        "ignored_words = set()\n",
        "for k, v in word_freq.items():\n",
        "    if word_freq[k] < MIN_WORD_FREQUENCY:\n",
        "        ignored_words.add(k)\n",
        "\n",
        "words = set(text_in_words)\n",
        "print('Unique words before ignoring:', len(words))\n",
        "print('Ignoring words with frequency <', MIN_WORD_FREQUENCY)\n",
        "words = sorted(set(words) - ignored_words)\n",
        "print('Unique words after ignoring:', len(words))\n",
        "print_vocabulary(vocabulary, words)\n",
        "\n",
        "word_indices = dict((c, i) for i, c in enumerate(words))\n",
        "indices_word = dict((i, c) for i, c in enumerate(words))\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus length in characters: 2436558\nCorpus length in words: 525131\nUnique words before ignoring: 29292\nIgnoring words with frequency < 1\nUnique words after ignoring: 29292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQRZmeh7_5jB"
      },
      "source": [
        "Entrenamiento y generacion de Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTwjFa9DWV1j",
        "outputId": "005faeab-bf72-4bc8-aa50-3ca196f32dcb"
      },
      "source": [
        "corpus = path + \"corpus_fs.txt\"\n",
        "\n",
        "with io.open(corpus, encoding='utf-8') as f:\n",
        "    text = f.read().lower().replace('\\n', ' \\n ')\n",
        "    text = text.replace('\\xa0', '')\n",
        "    text = re.sub('[.,:?¿¡!-]+','',text)\n",
        "\n",
        "text_in_words = [w for w in text.split(' ') if w.strip() != '' or w == '\\n']\n",
        "\n",
        "# cut the text in semi-redundant sequences of SEQUENCE_LEN words\n",
        "sentences = []\n",
        "next_words = []\n",
        "ignored = 0\n",
        "for i in range(0, len(text_in_words) - SEQUENCE_LEN, STEP):\n",
        "    # Only add the sequences where no word is in ignored_words\n",
        "    if len(set(text_in_words[i: i+SEQUENCE_LEN+1]).intersection(ignored_words)) == 0:\n",
        "        sentences.append(text_in_words[i: i + SEQUENCE_LEN])\n",
        "        next_words.append(text_in_words[i + SEQUENCE_LEN])\n",
        "    else:\n",
        "        ignored = ignored + 1\n",
        "print('Total sequences:', ignored + len(sentences))\n",
        "print('Ignored sequences:', ignored)\n",
        "print('Remaining sequences:', len(sentences))\n",
        "\n",
        "# x, y, x_test, y_test\n",
        "(sentences, next_words), (sentences_test, next_words_test) = shuffle_and_split_training_set(\n",
        "    sentences, next_words\n",
        ")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sequences: 83399\nIgnored sequences: 0\nRemaining sequences: 83399\nShuffling sentences\nSize of training set = 81731\nSize of test set = 1668\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2AKMnLK_zyx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cc8614ad-90a7-4826-e534-f5edba15fd39"
      },
      "source": [
        "\"\"\"\n",
        "Example script to train a network to generate text with the style of a given corpus\n",
        "--By word--\n",
        "It is recommended to run this script on GPU, as recurrent\n",
        "networks are quite computationally intensive.\n",
        "Based on\n",
        "https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
        "20 epochs should be enough to get decent results.\n",
        "Uses data generator to avoid loading all the test set into memory.\n",
        "Saves the weights and model every epoch.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import print_function\n",
        "from keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional\n",
        "import numpy as np\n",
        "import sys\n",
        "import json\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "BATCH_SIZE = 512\n",
        "\n",
        "\n",
        "# Data generator for fit and evaluate\n",
        "def generator(sentence_list, next_word_list, batch_size):\n",
        "    index = 0\n",
        "    while True:\n",
        "        x = np.zeros((batch_size, SEQUENCE_LEN, len(words)), dtype=np.bool)\n",
        "        y = np.zeros((batch_size, len(words)), dtype=np.bool)\n",
        "        for i in range(batch_size):\n",
        "            for t, w in enumerate(sentence_list[index % len(sentence_list)]):\n",
        "                x[i, t, word_indices[w]] = 1\n",
        "            y[i, word_indices[next_word_list[index % len(sentence_list)]]] = 1\n",
        "            index = index + 1\n",
        "        yield x, y\n",
        "\n",
        "\n",
        "\n",
        "def get_model(dropout=0.2):\n",
        "    print('Build model...')\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(256), input_shape=(SEQUENCE_LEN, len(words))))\n",
        "    if dropout > 0:\n",
        "        model.add(Dropout(dropout))\n",
        "    model.add(Dense(len(words)))\n",
        "    model.add(Activation('softmax'))\n",
        "    return model\n",
        "\n",
        "def get_custom_model(dropout=0.2):\n",
        "    print('Build custom model...')\n",
        "    inputs = Keras.Input(shape=(SEQUENCE_LENGTH, len(words)))\n",
        "    bidirectional = Bidirectional(LSTM(128))\n",
        "    x = bidirectional(inputs)\n",
        "    if dropout > 0:\n",
        "      x = Dropout(drouput)(x)\n",
        "    x = Dense(len(words))(x)\n",
        "    outputs = Activation('softmax')(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"custom_model\")\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "# Functions from keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "def on_epoch_end(epoch, logs):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    examples_file.write('\\n----- Generating text after Epoch: %d\\n' % epoch)\n",
        "\n",
        "    # Randomly pick a seed sequence\n",
        "    seed_index = np.random.randint(len(sentences+sentences_test))\n",
        "    seed = (sentences+sentences_test)[seed_index]\n",
        "\n",
        "    for diversity in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
        "        sentence = seed\n",
        "        examples_file.write('----- Diversity:' + str(diversity) + '\\n')\n",
        "        examples_file.write('----- Generating with seed:\\n\"' + ' '.join(sentence) + '\"\\n')\n",
        "        examples_file.write(' '.join(sentence))\n",
        "\n",
        "        for i in range(50):\n",
        "            x_pred = np.zeros((1, SEQUENCE_LEN, len(words)))\n",
        "            for t, word in enumerate(sentence):\n",
        "                x_pred[0, t, word_indices[word]] = 1.\n",
        "\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_word = indices_word[next_index]\n",
        "\n",
        "            sentence = sentence[1:]\n",
        "            sentence.append(next_word)\n",
        "\n",
        "            examples_file.write(\" \"+next_word)\n",
        "        examples_file.write('\\n')\n",
        "    examples_file.write('='*80 + '\\n')\n",
        "    examples_file.flush()\n",
        "\n",
        "\n",
        "model = get_model()\n",
        "if 'weights.h5' in os.listdir():\n",
        "  print('Weights loaded')\n",
        "  model.load_weights('weights.h5')\n",
        "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "file_path = \"./checkpoints/LSTM_LYRICS-epoch{epoch:03d}-words%d-sequence%d-minfreq%d-\" \\\n",
        "            \"loss{loss:.4f}-acc{accuracy:.4f}-val_loss{val_loss:.4f}-val_acc{val_accuracy:.4f}\" % \\\n",
        "            (len(words), SEQUENCE_LEN, MIN_WORD_FREQUENCY)\n",
        "\n",
        "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', save_best_only=True)\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "early_stopping = EarlyStopping(monitor='val_acc', patience=5)\n",
        "callbacks_list = [checkpoint, print_callback, early_stopping]\n",
        "\n",
        "examples_file = open(examples, \"w\")\n",
        "history = model.fit(generator(sentences, next_words, BATCH_SIZE),\n",
        "                    steps_per_epoch=int(len(sentences)/BATCH_SIZE) + 1,\n",
        "                    epochs=10,\n",
        "                    #callbacks=callbacks_list,\n",
        "                    validation_data=generator(sentences_test, next_words_test, BATCH_SIZE),\n",
        "                    validation_steps=int(len(sentences_test)/BATCH_SIZE) + 1)\n",
        "\n",
        "with io.open('history.json', 'w') as history_file:\n",
        "    json.dump(history.history, history_file)\n",
        "\n",
        "model.save_weights(\"weights.h5\")\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Build model...\n",
            "Weights loaded\n",
            "Epoch 1/10\n",
            "  1/160 [..............................] - ETA: 20:28 - loss: 0.7117 - accuracy: 0.8711"
          ]
        },
        {
          "output_type": "error",
          "ename": "InternalError",
          "evalue": "   Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 29292, 256, 1, 5, 512, 256] \n\t [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\n\t [[Adam/gradients/PartitionedCall_1]] [Op:__inference_train_function_5476]\n\nFunction call stack:\ntrain_function -> train_function -> train_function\n",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-3-2221ecc78a97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[0mexamples_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m history = model.fit(generator(sentences, next_words, BATCH_SIZE),\n\u001b[0m\u001b[0;32m    125\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mInternalError\u001b[0m:    Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 29292, 256, 1, 5, 512, 256] \n\t [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\n\t [[Adam/gradients/PartitionedCall_1]] [Op:__inference_train_function_5476]\n\nFunction call stack:\ntrain_function -> train_function -> train_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9gXFhwhtiaF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "5c0e62e4-2e94-4d41-9919-f5466a8468cd"
      },
      "source": [
        "model.save_weights('weights.h5')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_c3666ad9-33ad-4322-8b7b-01fb49df00ad\", \"weights.h5\", 302192080)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_0ebc20a0-1228-4063-a184-2e84474d2a0f\", \"history.json\", 3968)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KXyy-1PAMoq"
      },
      "source": [
        "Generacion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plCBzLnBACYS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "outputId": "33f01cf2-3e7e-4ac5-b7b9-585b2869c8a0"
      },
      "source": [
        "\"\"\"\n",
        "Script to generate text from an already trained network (with lstm_train.py)\n",
        "--By word--\n",
        "It is necessary to at least provide the trained model and the vocabulary file\n",
        "(generated also by lstm_train.py).\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "import re\n",
        "from keras.models import load_model\n",
        "\n",
        "def validate_seed(vocabulary, seed):\n",
        "    \"\"\"Validate that all the words in the seed are part of the vocabulary\"\"\"\n",
        "    print(\"\\nValidating that all the words in the seed are part of the vocabulary: \")\n",
        "    seed_words = seed.split(\" \")\n",
        "    valid = True\n",
        "    for w in seed_words:\n",
        "        print(w, end=\"\")\n",
        "        if w in vocabulary:\n",
        "            print(\" ✓ in vocabulary\")\n",
        "        else:\n",
        "            print(\" ✗ NOT in vocabulary\")\n",
        "            valid = False\n",
        "    return valid\n",
        "\n",
        "\n",
        "# Functions from keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "def rhyme_score(preds, previous_syllables):\n",
        "  #Previous syllables deberia ser la ultima silaba, o las dos ultimas silabas concatenadas\n",
        "    def rima(old_syl, next_syl):        \n",
        "        vowels = ['a','e','i','o','u','á', 'é', 'í', 'ó', 'ú']\n",
        "        if old_syl==next_syl:\n",
        "            return 0.8\n",
        "        old_strip=\"\".join(filter(lambda c: c in vowels, old_syl))\n",
        "        next_strip=\"\".join(filter(lambda c: c in vowels, next_syl))\n",
        "        return 0.7 if old_strip==next_strip else 0    \n",
        "    for index in range(len(preds)):\n",
        "        #if(preds[index]) > 0.8: #Modificar a verdaderas candidatas a sílaba\n",
        "        preds[index] += (1.0 - preds[index])*rima(previous_syllables,indices_word[index]) \n",
        "    return preds\n",
        "\n",
        "def generate_text(model, indices_word, word_indices, seed,\n",
        "                  sequence_length, diversity, quantity):\n",
        "    \"\"\"\n",
        "    Similar to lstm_train::on_epoch_end\n",
        "    Used to generate text using a trained model\n",
        "\n",
        "    :param model: the trained Keras model (with model.load)\n",
        "    :param indices_word: a dictionary pointing to the words\n",
        "    :param seed: a string to be used as seed (already validated and padded)\n",
        "    :param sequence_length: how many words are given to the model to generate\n",
        "    :param diversity: is the \"temperature\" of the sample function (usually between 0.1 and 2)\n",
        "    :param quantity: quantity of words to generate\n",
        "    :return: Nothing, for now only writes the text to console\n",
        "    \"\"\"\n",
        "    sentence = seed.split(\" \")\n",
        "    print(\"----- Generating text\")\n",
        "    print('----- Diversity:' + str(diversity))\n",
        "    print('----- Generating with seed:\\n\"' + seed)\n",
        "\n",
        "    print()\n",
        "    print(seed)\n",
        "\n",
        "    prev_preds = []\n",
        "    text = []\n",
        "    last_syllables = []\n",
        "    elegidas = []\n",
        "    elegidas_text = []\n",
        "    rhyme_time=0\n",
        "    for i in range(quantity):\n",
        "        x_pred = np.zeros((1, sequence_length, len(vocabulary)))\n",
        "        for t, word in enumerate(sentence):\n",
        "            x_pred[0, t, word_indices[word]] = 1.\n",
        "\n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        # preds tiene arreglo resultado de softmax\n",
        "\n",
        "        next_index = sample(preds, diversity)\n",
        "        next_word = indices_word[next_index]\n",
        "\n",
        "        if next_word == '\\n' and len(text) > 0:\n",
        "          # redefinir sentence[-1] usando prev_preds (valor de la prediccion anterior)\n",
        "          \n",
        "          text[-1] = text[-1].upper()\n",
        "          \n",
        "          if (text[-1] != '\\n'):\n",
        "            silaba_reemplazada = text[-1]\n",
        "            if last_syllables != [] and (text[-1]!='\\\\' or text[-1]!='\\n'):# and (rhyme_time%2==0):              \n",
        "              changed_prev_index = sample(rhyme_score(prev_preds, last_syllables[-1][-1]))\n",
        "              new_word = indices_word[changed_prev_index]\n",
        "              text[-1] = new_word\n",
        "              sentence[-1] = new_word\n",
        "              elegidas_text.append((new_word,silaba_reemplazada))\n",
        "              #rhyme_score+=1\n",
        "\n",
        "            last_syllables.append(list(filter(lambda x: (x!='\\\\'),sentence[-2:])))\n",
        "          else:\n",
        "            last_syllables = []\n",
        "\n",
        "        sentence = sentence[1:]\n",
        "        sentence.append(next_word)\n",
        "        text.append(next_word)\n",
        "        prev_word = next_word\n",
        "        prev_preds = preds\n",
        "\n",
        "    print(\"Pares de silabas reemplazadas, silabas originales:\")\n",
        "    print(elegidas_text)\n",
        "    print()\n",
        "    return text\n",
        "\n",
        "vocabulary_file = \"vocabulary.txt\"\n",
        "#model_file = args.network\n",
        "seed = \"el barrio cuenta que dices\"\n",
        "sequence_length = SEQUENCE_LEN #args.sequence_length\n",
        "diversity = 0.5#args.diversity\n",
        "quantity = 5000 #args.quantity\n",
        "\n",
        "#if not vocabulary_file or not model_file:\n",
        "#    print('\\033[91mERROR: At least --vocabulary and --network are needed\\033[0m')\n",
        "#    exit(0)\n",
        "\n",
        "#model = load_model(model_file)\n",
        "print(\"\\nSummary of the Network: \")\n",
        "model.summary()\n",
        "\n",
        "vocabulary = open(vocabulary_file, \"r\", encoding='utf-8').readlines()\n",
        "# remove the \\n at the end of the word, except for the \\n word itself\n",
        "vocabulary = [re.sub(r'(\\S+)\\s+', r'\\1', w) for w in vocabulary]\n",
        "vocabulary = sorted(set(vocabulary))\n",
        "\n",
        "word_indices = dict((c, i) for i, c in enumerate(vocabulary))\n",
        "indices_word = dict((i, c) for i, c in enumerate(vocabulary))\n",
        "\n",
        "if validate_seed(vocabulary, seed):\n",
        "    print(\"\\nSeed is correct.\\n\")\n",
        "    # repeat the seed in case is not long enough, and take only the last elements\n",
        "    seed = \" \".join((((seed+\" \")*sequence_length)+seed).split(\" \")[-sequence_length:])\n",
        "    text = generate_text(\n",
        "        model, indices_word, word_indices, seed, sequence_length, diversity, quantity\n",
        "    )\n",
        "    with open('generated2.txt', \"w\") as text_file:\n",
        "      for word in text: \n",
        "        printword = word+\" \" if word != '\\\\' else ' '\n",
        "        #print(printword)\n",
        "        text_file.write(printword)\n",
        "    print(printword)\n",
        "else:\n",
        "    print('\\033[91mERROR: Please fix the seed string\\033[0m')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFvR5ED_L9iT"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}